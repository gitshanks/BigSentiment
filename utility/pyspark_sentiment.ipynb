{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "'''\n",
    "Purpose: Use the prebuilt Vader Sentiment Model from the nltk library to assign sentiment labels to tweets or \n",
    "Reddit posts as they are streamed through Spark. This is the implementation we ultimately used.\n",
    "'''\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "         .appName(\"spark-nltk\") \\\n",
    "         .getOrCreate()\n",
    " \n",
    "data = spark.sparkContext.textFile('test.txt')\n",
    " \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "def assign_sentiment(x):\n",
    "    return dict(sent_analyzer.polarity_scores(x))\n",
    " \n",
    "score = data.map(assign_sentiment)\n",
    "print(score.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Purpose: Build a custom machine learning model with PySpark for analyzing sentiment from the Twitter 140 traning set.\n",
    "This idea was ultimately abandoned for the one above. \n",
    "'''\n",
    "\n",
    "df = pd.read_csv('/Users/mchifala/atls-5214-project/Big_Sentiment/utility/twitter_sentiment_training_set.csv', \n",
    "            header = None, \n",
    "            encoding = 'ISO-8859-1', \n",
    "            names = ['label', 'id', 'date', '?', 'author', 'text'])     \n",
    "\n",
    "df['words'] = df['text'].apply(lambda x: re.sub(r'http\\S+|@\\S+|[?|$|.|!|,|;|:|_|-|*|~|/|\\|+|=|(|)|#]','',x))\n",
    "df['words'] = df['words'].apply(lambda x: x.lower().split())\n",
    "\n",
    "stop = ['i', \"i'm\", \"i've\", 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \n",
    "        \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \n",
    "        'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
    "        'which', 'who', 'whom', 'this', \"that's\", 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "        'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "        'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', \n",
    "        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "        'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how']\n",
    "\n",
    "df['words'] = df['words'].apply(lambda x: [word for word in x if word not in stop])\n",
    "df['clean_text'] = df['words'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df = df.drop(['id', 'date', '?', 'author', 'text', 'words'], axis = 'columns')\n",
    "#df = df.drop('words', axis = 'columns')\n",
    "df.to_csv('clean_twitter_sentiment_training_set.csv', encoding = 'utf-8', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_compound_sentiment'] = df['full_sentiment'].apply(lambda x: x['compound'])\n",
    "df.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder.appName('Sentiment').getOrCreate()\n",
    "\n",
    "tweetDF = spark.read\\\n",
    "        .option(\"encoding\", \"UTF-8\")\\\n",
    "        .csv('/Users/mchifala/atls-5214-project/Big_Sentiment/utility/clean_twitter_sentiment_training_set.csv', header=True, inferSchema=True)\n",
    "\n",
    "tweetDF.dropna()\n",
    "tweetDF.show(20, False)\n",
    "\n",
    "tweetDF.dropna()\n",
    "\n",
    "(train_set, test_set) = tweetDF.randomSplit([0.9, 0.1], seed = 5)\n",
    "\n",
    "train_set = train_set.filter(train_set.clean_text.isNotNull())\n",
    "train_set.show(5000)\n",
    "print(test_set.count())\n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=3)\n",
    "\n",
    "train_set.show()\n",
    "pipeline = Pipeline(stages=[tokenizer])#, hashtf])#, idf])\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_set = pipelineFit.transform(train_set)\n",
    "\n",
    "train_set.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
