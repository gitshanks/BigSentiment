{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "from requests_oauthlib import OAuth1\n",
    "import oauth2 as oauth\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import csv\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer\n",
    "from json import dumps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials from local source:\n",
    "reader = csv.reader(open(\"/Users/mchifala/Desktop/ATLS_5412/Credentials.csv\"))\n",
    "credentials = {}\n",
    "for line in reader:\n",
    "    credentials[line[0]] = line[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Reddit credentials for API:\n",
    "def getRedditHeaders():\n",
    "    reddit_id = credentials['reddit_id']\n",
    "    reddit_secret = credentials['reddit_secret']\n",
    "    reddit_username = credentials['reddit_username']\n",
    "    reddit_password = credentials['reddit_password']\n",
    "    reddit_user_agent = credentials['reddit_user_agent']\n",
    "\n",
    "    reddit_auth = requests.auth.HTTPBasicAuth(reddit_id, reddit_secret)\n",
    "    reddit_post_data = {'grant_type': 'password', 'username': reddit_username, 'password': reddit_password}\n",
    "    reddit_token_headers = {'User-Agent': reddit_user_agent}\n",
    "    r_auth = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth= reddit_auth, data=reddit_post_data, headers=reddit_token_headers)\n",
    "\n",
    "    reddit_token = r_auth.json()['token_type'] + ' ' + r_auth.json()['access_token']\n",
    "    \n",
    "    # Return header dictionary\n",
    "    return {\"Authorization\": reddit_token, \"User-Agent\": reddit_user_agent, 'Content-Type': 'application/json'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Twitter credentials:\n",
    "def getTwitterAuth():\n",
    "    twitter_key = credentials['twitter_key']\n",
    "    twitter_secret =  credentials['twitter_secret']\n",
    "    twitter_token = credentials['twitter_token']\n",
    "    twitter_secret_token = credentials['twitter_secret_token']\n",
    "    \n",
    "    #Return authorization object\n",
    "    return OAuth1(twitter_key, twitter_secret, twitter_token, twitter_secret_token)\n",
    "\n",
    "getTwitterAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all available Twitter trends:\n",
    "twitter_auth = getTwitterAuth()\n",
    "r_trends = requests.get('https://api.twitter.com/1.1/trends/available.json', auth = twitter_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the location ID codes for trends:\n",
    "world_codes = []\n",
    "for i in range(0,len(r_trends.json())):\n",
    "    world_codes.append(r_trends.json()[i]['woeid'])\n",
    "\n",
    "test_codes = world_codes[20:30]\n",
    "print(test_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search each location ID for its specific trends:\n",
    "topics = []\n",
    "for code in world_codes:\n",
    "    r_trends2 = requests.get('https://api.twitter.com/1.1/trends/place.json?id='+str(code), auth = twitter_auth)\n",
    "    for i in range(0,len(r_trends2.json()[0])):\n",
    "        print(r_trends2.json()[0]['trends'][i]['name'])\n",
    "        topics.append((r_trends2.json()[0]['trends'][i]['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to control the number of calls to Reddit API; sleep when limit is exceeded\n",
    "def redditSearch(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, trends):\n",
    "    for query in set(trends):\n",
    "        if (int(float(ratelimit_calls)) > 0):\n",
    "            #print('Before call')\n",
    "            #print(ratelimit_calls, ratelimit_reset)\n",
    "            ratelimit_calls, ratelimit_reset = redditQuery(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, query)  \n",
    "            #print('After call')\n",
    "            #print(ratelimit_calls, ratelimit_reset)\n",
    "        else:\n",
    "            print('Sleep')\n",
    "            time.sleep(int(ratelimit_reset))\n",
    "            ratelimit_calls, ratelimit_reset = redditQuery(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original function for publishing directly to Elasticsearch\n",
    "def redditQuery(reddit_headers,elastic_headers,elastic_index, ratelimit_calls, ratelimit_reset, query):\n",
    "    r_get = requests.get('https://oauth.reddit.com/r/all/search?q='+query+'&t=day&limit=25', headers = reddit_headers)\n",
    "    posts = r_get.json()['data']['children']\n",
    "    \n",
    "    data = []\n",
    "    fields_to_keep = ['id','title', 'author', 'score', 'subreddit', 'num_comments', 'created_utc', 'url']\n",
    "    for post in posts:\n",
    "        temp = {k: post['data'][k] for k in fields_to_keep}\n",
    "        temp.update({'hashtag': query, 'source': 'reddit'})\n",
    "        temp['post_date'] = datetime.isoformat(datetime.utcfromtimestamp(temp['created_utc']))\n",
    "        temp['upvotes'] = temp['score']\n",
    "        data.append({'index': {\"_index\": elastic_index, '_type': '_doc', '_id': 'r_'+ temp['id'] }})\n",
    "        url_split = temp['url'].split('.')[-1]\n",
    "        if (url_split != \"jpg\" and url_split != \"png\"):\n",
    "            del temp['url']\n",
    "        del temp['id']\n",
    "        del temp['created_utc']\n",
    "        del temp['score']\n",
    "        data.append(temp)\n",
    "\n",
    "    if data:\n",
    "        data_to_post = '\\n'.join(json.dumps(d) for d in data)\n",
    "        print(data_to_post)\n",
    "        r_post = requests.post('http://34.73.60.209:9200/'+elastic_index+'/_bulk', headers = elastic_headers, data=data_to_post+'\\n')\n",
    "        print(r_post.text)\n",
    "    \n",
    "    print(r_get.headers['x-ratelimit-remaining'], r_get.headers['x-ratelimit-reset'])\n",
    "    \n",
    "    return r_get.headers['x-ratelimit-remaining'], r_get.headers['x-ratelimit-reset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised function for publishing to Kakfa-Spark-Elasticsearch pipeline\n",
    "def redditQuery(reddit_headers,elastic_headers,elastic_index, ratelimit_calls, ratelimit_reset, query, producer, topic):\n",
    "    r_get = requests.get('https://oauth.reddit.com/r/all/search?q='+query+'&t=day&limit=25', headers = reddit_headers)\n",
    "    posts = r_get.json()['data']['children']\n",
    "    \n",
    "    fields_to_keep = ['id','title', 'author', 'score', 'subreddit', 'num_comments', 'created_utc', 'url']\n",
    "    for post in posts:\n",
    "        \n",
    "        data = {k: post['data'][k] for k in fields_to_keep}\n",
    "        time = datetime.isoformat(datetime.utcfromtimestamp(data['created_utc']))\n",
    "        data.update({'hashtag': query, 'source': 'reddit', 'id': 'r_'+ data['id'], 'upvotes': data['score'], 'post_date': time })    \n",
    "        url_split = data['url'].split('.')[-1]\n",
    "        if (url_split != \"jpg\" and url_split != \"png\"):\n",
    "            del data['url']\n",
    "            \n",
    "        del data['created_utc']\n",
    "        del data['score']\n",
    "        \n",
    "        producer.send(topic, value=data)\n",
    "\n",
    "    return r_get.headers['x-ratelimit-remaining'], r_get.headers['x-ratelimit-reset']\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['35.232.117.118:9092'], \n",
    "                        value_serializer=lambda x: dumps(x).encode('utf-8'),\n",
    "                        api_version = (0,10))\n",
    "topic = 'trending'\n",
    "redditQuery(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, query, producer, topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Enviroment for Rate Limits:\n",
    "reddit_headers = getRedditHeaders()\n",
    "elastic_headers = {'Content-Type':'application/json'}\n",
    "elastic_index = 'hi_yash2'\n",
    "\n",
    "r_get = requests.get('https://oauth.reddit.com', headers = reddit_headers)\n",
    "ratelimit_calls =  r_get.headers['x-ratelimit-remaining']\n",
    "ratelimit_reset = r_get.headers['x-ratelimit-reset']\n",
    "\n",
    "redditSearch(reddit_headers, elastic_headers, elastic_index, ratelimit_calls, ratelimit_reset, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a working example of bulk uploads to Elasticsearch: make sure the indexes match\n",
    "data = [\n",
    "    { \"index\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"1\" } },\n",
    "    { \"field1\" : \"value1\" },\n",
    "    { \"delete\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"2\" }, },\n",
    "    { \"create\" : { \"_index\" : \"test5\", \"_type\" : \"type1\", \"_id\" : \"3\" }, },\n",
    "    { \"field1\" : \"value3\" },\n",
    "    { \"update\" : {\"_id\" : \"1\", \"_type\" : \"type1\", \"_index\" : \"test5\"} },\n",
    "    { \"doc\" : {\"field2\" : \"value2\"} }\n",
    "]\n",
    "\n",
    "data_to_post = '\\n'.join(json.dumps(d) for d in data)\n",
    "print(data_to_post)\n",
    "r = requests.post('http://34.73.60.209:9200/test5/_bulk', headers = headers, data=data_to_post + '\\n')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a working example of a single record uplaod to Elasticsearch:\n",
    "requests.post('http://34.73.60.209:9200/hi_yash/_doc/',  json = {\"user\" : \"mchifala\",\n",
    "  \"post_date\" : \"2019-03-10T15:41:12\",\n",
    "  \"message\" : \"Hi Yash\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
